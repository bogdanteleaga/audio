{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# CTC forced alignment API tutorial\n\n**Author**: [Xiaohui Zhang](xiaohuizhang@meta.com)_\n\n\nThis tutorial shows how to align transcripts to speech with\n``torchaudio``'s CTC forced alignment API proposed in the paper\n[\u201cScaling Speech Technology to 1,000+\nLanguages\u201d](https://research.facebook.com/publications/scaling-speech-technology-to-1000-languages/)_,\nand one advanced usage, i.e.\u00a0dealing with transcription errors with a <star> token.\n\nThough there\u2019s some overlap in visualization\ndiagrams, the scope here is different from the [\u201cForced Alignment with\nWav2Vec2\u201d](https://pytorch.org/audio/stable/tutorials/forced_alignment_tutorial.html)_\ntutorial, which focuses on a step-by-step demonstration of the forced\nalignment generation algorithm (without using an API) described in the\n[paper](https://arxiv.org/abs/2007.09127)_ with a Wav2Vec2 model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchaudio\n\nprint(torch.__version__)\nprint(torchaudio.__version__)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n\ntry:\n    from torchaudio.functional import forced_align\nexcept ModuleNotFoundError:\n    print(\n        \"Failed to import the forced alignment API. \"\n        \"Please install torchaudio nightly builds. \"\n        \"Please refer to https://pytorch.org/get-started/locally \"\n        \"for instructions to install a nightly build.\"\n    )\n    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic usages\n\nIn this section, we cover the following content:\n\n1. Generate frame-wise class probabilites from audio waveform from a CTC\n   acoustic model.\n2. Compute frame-level alignments using TorchAudio\u2019s forced alignment\n   API.\n3. Obtain token-level alignments from frame-level alignments.\n4. Obtain word-level alignments from token-level alignments.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparation\n\nFirst we import the necessary packages, and fetch data that we work on.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# %matplotlib inline\nfrom dataclasses import dataclass\n\nimport IPython\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nmatplotlib.rcParams[\"figure.figsize\"] = [16.0, 4.8]\n\ntorch.random.manual_seed(0)\n\nSPEECH_FILE = torchaudio.utils.download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\")\nsample_rate = 16000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate frame-wise class posteriors from a CTC acoustic model\n\nThe first step is to generate the class probabilities (i.e.\u00a0posteriors)\nof each audio frame using a CTC model.\nHere we use :py:func:`torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\nmodel = bundle.get_model().to(device)\nlabels = bundle.get_labels()\nwith torch.inference_mode():\n    waveform, _ = torchaudio.load(SPEECH_FILE)\n    emissions, _ = model(waveform.to(device))\n    emissions = torch.log_softmax(emissions, dim=-1)\n\nemission = emissions.cpu().detach()\ndictionary = {c: i for i, c in enumerate(labels)}\n\nprint(dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualization\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.imshow(emission[0].T)\nplt.colorbar()\nplt.title(\"Frame-wise class probabilities\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Labels\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Computing frame-level alignments\n\nThen we call TorchAudio\u2019s forced alignment API to compute the\nframe-level alignment between each audio frame and each token in the\ntranscript. We first explain the inputs and outputs of the API\n``functional.forced_align``. Note that this API works on both CPU and\nGPU. In the current tutorial we demonstrate it on CPU.\n\n**Inputs**:\n\n``emission``: a 2D tensor of size $T \\times N$, where $T$ is\nthe number of frames (after sub-sampling by the acoustic model, if any),\nand $N$ is the vocabulary size.\n\n``targets``: a 1D tensor vector of size $M$, where $M$ is\nthe length of the transcript, and each element is a token ID looked up\nfrom the vocabulary. For example, the ``targets`` tensor repsenting the\ntranscript \u201ci had\u2026\u201d is $[5, 18, 4, 16, ...]$.\n\n``input lengths``: $T$.\n\n``target lengths``: $M$.\n\n**Outputs**:\n\n``frame_alignment``: a 1D tensor of size $T$ storing the aligned\ntoken index (looked up from the vocabulary) of each frame, e.g.\u00a0for the\nsegment corresponding to \u201ci had\u201d in the given example , the\nframe_alignment is\n$[...0, 0, 5, 0, 0, 18, 18, 4, 0, 0, 0, 16,...]$, where $0$\nrepresents the blank symbol.\n\n``frame_scores``: a 1D tensor of size $T$ storing the confidence\nscore (0 to 1) for each each frame. For each frame, the score should be\nclose to one if the alignment quality is good.\n\nFrom the outputs ``frame_alignment`` and ``frame_scores``, we generate a\nlist called ``frames`` storing information of all frames aligned to\nnon-blank tokens. Each element contains 1) token_index: the aligned\ntoken\u2019s index in the transcript 2) time_index: the current frame\u2019s index\nin the input audio (or more precisely, the row dimension of the emission\nmatrix) 3) the confidence scores of the current frame.\n\nFor the given example, the first few elements of the list ``frames``\ncorresponding to \u201ci had\u201d looks as the following:\n\n``Frame(token_index=0, time_index=32, score=0.9994410872459412)``\n\n``Frame(token_index=1, time_index=35, score=0.9980823993682861)``\n\n``Frame(token_index=1, time_index=36, score=0.9295750260353088)``\n\n``Frame(token_index=2, time_index=37, score=0.9997448325157166)``\n\n``Frame(token_index=3, time_index=41, score=0.9991760849952698)``\n\n``...``\n\nThe interpretation is:\n\nThe token with index $0$ in the transcript, i.e.\u00a0\u201ci\u201d, is aligned\nto the $32$\\ th audio frame, with confidence $0.9994$. The\ntoken with index $1$ in the transcript, i.e.\u00a0\u201ch\u201d, is aligned to\nthe $35$\\ th and $36$\\ th audio frames, with confidence\n$0.9981$ and $0.9296$ respectively. The token with index\n$2$ in the transcript, i.e.\u00a0\u201ca\u201d, is aligned to the $35$\\ th\nand $36$\\ th audio frames, with confidence $0.9997$. The\ntoken with index $3$ in the transcript, i.e.\u00a0\u201cd\u201d, is aligned to\nthe $41$\\ th audio frame, with confidence $0.9992$.\n\nFrom such information stored in the ``frames`` list, we\u2019ll compute\ntoken-level and word-level alignments easily.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@dataclass\nclass Frame:\n    # This is the index of each token in the transcript,\n    # i.e. the current frame aligns to the N-th character from the transcript.\n    token_index: int\n    time_index: int\n    score: float\n\n\ndef compute_alignments(transcript, dictionary, emission):\n    frames = []\n    tokens = [dictionary[c] for c in transcript.replace(\" \", \"\")]\n\n    targets = torch.tensor(tokens, dtype=torch.int32).unsqueeze(0)\n    input_lengths = torch.tensor([emission.shape[1]])\n    target_lengths = torch.tensor([targets.shape[1]])\n\n    # This is the key step, where we call the forced alignment API functional.forced_align to compute alignments.\n    frame_alignment, frame_scores = forced_align(emission, targets, input_lengths, target_lengths, 0)\n\n    assert frame_alignment.shape[1] == input_lengths[0].item()\n    assert targets.shape[1] == target_lengths[0].item()\n\n    token_index = -1\n    prev_hyp = 0\n    for i in range(frame_alignment.shape[1]):\n        if frame_alignment[0][i].item() == 0:\n            prev_hyp = 0\n            continue\n\n        if frame_alignment[0][i].item() != prev_hyp:\n            token_index += 1\n        frames.append(Frame(token_index, i, frame_scores[0][i].exp().item()))\n        prev_hyp = frame_alignment[0][i].item()\n    return frames, frame_alignment, frame_scores\n\n\ntranscript = \"I|HAD|THAT|CURIOSITY|BESIDE|ME|AT|THIS|MOMENT\"\nframes, frame_alignment, frame_scores = compute_alignments(transcript, dictionary, emission)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Obtain token-level alignments and confidence scores\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The frame-level alignments contains repetations for the same labels.\nAnother format \u201ctoken-level alignment\u201d, which specifies the aligned\nframe ranges for each transcript token, contains the same information,\nwhile being more convenient to apply to some downstream tasks\n(e.g.\u00a0computing word-level alignments).\n\nNow we demonstrate how to obtain token-level alignments and confidence\nscores by simply merging frame-level alignments and averaging\nframe-level confidence scores.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Merge the labels\n@dataclass\nclass Segment:\n    label: str\n    start: int\n    end: int\n    score: float\n\n    def __repr__(self):\n        return f\"{self.label}\\t({self.score:4.2f}): [{self.start:5d}, {self.end:5d})\"\n\n    @property\n    def length(self):\n        return self.end - self.start\n\n\ndef merge_repeats(frames, transcript):\n    transcript_nospace = transcript.replace(\" \", \"\")\n    i1, i2 = 0, 0\n    segments = []\n    while i1 < len(frames):\n        while i2 < len(frames) and frames[i1].token_index == frames[i2].token_index:\n            i2 += 1\n        score = sum(frames[k].score for k in range(i1, i2)) / (i2 - i1)\n        segments.append(\n            Segment(\n                transcript_nospace[frames[i1].token_index],\n                frames[i1].time_index,\n                frames[i2 - 1].time_index + 1,\n                score,\n            )\n        )\n        i1 = i2\n    return segments\n\n\nsegments = merge_repeats(frames, transcript)\nfor seg in segments:\n    print(seg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualization\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_label_prob(segments, transcript):\n    fig, ax2 = plt.subplots(figsize=(16, 4))\n\n    ax2.set_title(\"frame-level and token-level confidence scores\")\n    xs, hs, ws = [], [], []\n    for seg in segments:\n        if seg.label != \"|\":\n            xs.append((seg.end + seg.start) / 2 + 0.4)\n            hs.append(seg.score)\n            ws.append(seg.end - seg.start)\n            ax2.annotate(seg.label, (seg.start + 0.8, -0.07), weight=\"bold\")\n    ax2.bar(xs, hs, width=ws, color=\"gray\", alpha=0.5, edgecolor=\"black\")\n\n    xs, hs = [], []\n    for p in frames:\n        label = transcript[p.token_index]\n        if label != \"|\":\n            xs.append(p.time_index + 1)\n            hs.append(p.score)\n\n    ax2.bar(xs, hs, width=0.5, alpha=0.5)\n    ax2.axhline(0, color=\"black\")\n    ax2.set_ylim(-0.1, 1.1)\n\n\nplot_label_prob(segments, transcript)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the visualized scores, we can see that, for tokens spanning over\nmore multiple frames, e.g.\u00a0\u201cT\u201d in \u201cTHAT, the token-level confidence\nscore is the average of frame-level confidence scores. To make this\nclearer, we don\u2019t plot confidence scores for blank frames, which was\nplotted in the\u201dLabel probability with and without repeatation\u201d figure in\nthe previous tutorial [\u201cForced Alignment with\nWav2Vec2\u201d](https://pytorch.org/audio/stable/tutorials/forced_alignment_tutorial.html)_.\n\n### Obtain word-level alignments and confidence scores\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let\u2019s merge the token-level alignments and confidence scores to get\nword-level alignments and confidence scores. Then, finally, we verify\nthe quality of word alignments by 1) plotting the word-level alignments\nand the waveform, 2) segmenting the original audio according to the\nalignments and listening to them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Obtain word alignments from token alignments\ndef merge_words(transcript, segments, separator=\" \"):\n    words = []\n    i1, i2, i3 = 0, 0, 0\n    while i3 < len(transcript):\n        if i3 == len(transcript) - 1 or transcript[i3] == separator:\n            if i1 != i2:\n                if i3 == len(transcript) - 1:\n                    i2 += 1\n                if separator == \"|\":\n                    # s is the number of separators (counted as a valid modeling unit) we've seen\n                    s = len(words)\n                else:\n                    s = 0\n                segs = segments[i1 + s : i2 + s]\n                word = \"\".join([seg.label for seg in segs])\n                score = sum(seg.score * seg.length for seg in segs) / sum(seg.length for seg in segs)\n                words.append(Segment(word, segments[i1 + s].start, segments[i2 + s - 1].end, score))\n            i1 = i2\n        else:\n            i2 += 1\n        i3 += 1\n    return words\n\n\nword_segments = merge_words(transcript, segments, \"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualization\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_alignments(segments, word_segments, waveform, input_lengths, scale=10):\n    fig, ax2 = plt.subplots(figsize=(64, 12))\n    plt.rcParams.update({\"font.size\": 30})\n\n    # The original waveform\n    ratio = waveform.size(1) / input_lengths\n    ax2.plot(waveform)\n    ax2.set_ylim(-1.0 * scale, 1.0 * scale)\n    ax2.set_xlim(0, waveform.size(-1))\n\n    for word in word_segments:\n        x0 = ratio * word.start\n        x1 = ratio * word.end\n        ax2.axvspan(x0, x1, alpha=0.1, color=\"red\")\n        ax2.annotate(f\"{word.score:.2f}\", (x0, 0.8 * scale))\n\n    for seg in segments:\n        if seg.label != \"|\":\n            ax2.annotate(seg.label, (seg.start * ratio, 0.9 * scale))\n\n    xticks = ax2.get_xticks()\n    plt.xticks(xticks, xticks / sample_rate, fontsize=50)\n    ax2.set_xlabel(\"time [second]\", fontsize=40)\n    ax2.set_yticks([])\n\n\nplot_alignments(\n    segments,\n    word_segments,\n    waveform,\n    emission.shape[1],\n    1,\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# A trick to embed the resulting audio to the generated file.\n# `IPython.display.Audio` has to be the last call in a cell,\n# and there should be only one call par cell.\ndef display_segment(i, waveform, word_segments, frame_alignment):\n    ratio = waveform.size(1) / frame_alignment.size(1)\n    word = word_segments[i]\n    x0 = int(ratio * word.start)\n    x1 = int(ratio * word.end)\n    print(f\"{word.label} ({word.score:.2f}): {x0 / sample_rate:.3f} - {x1 / sample_rate:.3f} sec\")\n    segment = waveform[:, x0:x1]\n    return IPython.display.Audio(segment.numpy(), rate=sample_rate)\n\n\n# Generate the audio for each segment\nprint(transcript)\nIPython.display.Audio(SPEECH_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(0, waveform, word_segments, frame_alignment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(1, waveform, word_segments, frame_alignment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(2, waveform, word_segments, frame_alignment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(3, waveform, word_segments, frame_alignment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(4, waveform, word_segments, frame_alignment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(5, waveform, word_segments, frame_alignment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(6, waveform, word_segments, frame_alignment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(7, waveform, word_segments, frame_alignment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(8, waveform, word_segments, frame_alignment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced usage: Dealing with missing transcripts using the <star> token\n\nNow let\u2019s look at when the transcript is partially missing, how can we\nimprove alignment quality using the <star> token, which is capable of modeling\nany token.\n\nHere we use the same English example as used above. But we remove the\nbeginning text \u201ci had that curiosity beside me at\u201d from the transcript.\nAligning audio with such transcript results in wrong alignments of the\nexisting word \u201cthis\u201d. However, this issue can be mitigated by using the\n<star> token to model the missing text.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Reload the emission tensor in order to add the extra dimension corresponding to the <star> token.\nwith torch.inference_mode():\n    waveform, _ = torchaudio.load(SPEECH_FILE)\n    emissions, _ = model(waveform.to(device))\n    emissions = torch.log_softmax(emissions, dim=-1)\n\n    # Append the extra dimension corresponding to the <star> token\n    extra_dim = torch.zeros(emissions.shape[0], emissions.shape[1], 1)\n    emissions = torch.cat((emissions.cpu(), extra_dim), 2)\n    emission = emissions.detach()\n\n# Extend the dictionary to include the <star> token.\ndictionary[\"*\"] = 29\n\nassert len(dictionary) == emission.shape[2]\n\n\ndef compute_and_plot_alignments(transcript, dictionary, emission, waveform):\n    frames, frame_alignment, _ = compute_alignments(transcript, dictionary, emission)\n    segments = merge_repeats(frames, transcript)\n    word_segments = merge_words(transcript, segments, \"|\")\n    plot_alignments(segments, word_segments, waveform, emission.shape[1], 1)\n    plt.show()\n    return word_segments, frame_alignment\n\n\n# original:\nword_segments, frame_alignment = compute_and_plot_alignments(transcript, dictionary, emission, waveform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Demonstrate the effect of <star> token for dealing with deletion errors\n# (\"i had that curiosity beside me at\" missing from the transcript):\ntranscript = \"THIS|MOMENT\"\nword_segments, frame_alignment = compute_and_plot_alignments(transcript, dictionary, emission, waveform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Replacing the missing transcript with the <star> token:\ntranscript = \"*|THIS|MOMENT\"\nword_segments, frame_alignment = compute_and_plot_alignments(transcript, dictionary, emission, waveform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this tutorial, we looked at how to use torchaudio\u2019s forced alignment\nAPI to align and segment speech files, and demonstrated one advanced usage:\nHow introducing a <star> token could improve alignment accuracy when\ntranscription errors exist.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Acknowledgement\n\nThanks to [Vineel Pratap](vineelkpratap@meta.com)_ and [Zhaoheng\nNi](zni@meta.com)_ for working on the forced aligner API, and [Moto\nHira](moto@meta.com)_ for providing alignment merging and\nvisualization utilities.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}