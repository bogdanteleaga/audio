{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Forced alignment for multilingual data\n\n**Author**: [Xiaohui Zhang](xiaohuizhang@meta.com)_\n\nThis tutorial shows how to compute forced alignments for speech data\nfrom multiple non-English languages using ``torchaudio``'s CTC forced alignment\nAPI described in [\u201cCTC forced alignment\ntutorial\u201d](https://pytorch.org/audio/stable/tutorials/forced_alignment_tutorial.html)_\nand the multilingual Wav2vec2 model proposed in the paper [\u201cScaling\nSpeech Technology to 1,000+\nLanguages\u201d](https://research.facebook.com/publications/scaling-speech-technology-to-1000-languages/)_.\nThe model was trained on 23K of audio data from 1100+ languages using\nthe [\u201curoman vocabulary\u201d](https://www.isi.edu/~ulf/uroman.html)_\nas targets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchaudio\n\nprint(torch.__version__)\nprint(torchaudio.__version__)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n\ntry:\n    from torchaudio.functional import forced_align\nexcept ModuleNotFoundError:\n    print(\n        \"Failed to import the forced alignment API. \"\n        \"Please install torchaudio nightly builds. \"\n        \"Please refer to https://pytorch.org/get-started/locally \"\n        \"for instructions to install a nightly build.\"\n    )\n    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparation\n\nHere we import necessary packages, and define utility functions for\ncomputing the frame-level alignments (using the API\n``functional.forced_align``), token-level and word-level alignments, and\nalso alignment visualization utilities.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# %matplotlib inline\nfrom dataclasses import dataclass\n\nimport IPython\n\nimport matplotlib.pyplot as plt\n\ntorch.random.manual_seed(0)\n\nsample_rate = 16000\n\n\n@dataclass\nclass Frame:\n    # This is the index of each token in the transcript,\n    # i.e. the current frame aligns to the N-th character from the transcript.\n    token_index: int\n    time_index: int\n    score: float\n\n\n@dataclass\nclass Segment:\n    label: str\n    start: int\n    end: int\n    score: float\n\n    def __repr__(self):\n        return f\"{self.label}\\t({self.score:4.2f}): [{self.start:5d}, {self.end:5d})\"\n\n    @property\n    def length(self):\n        return self.end - self.start\n\n\n# compute frame-level and word-level alignments using torchaudio's forced alignment API\ndef compute_alignments(transcript, dictionary, emission):\n    frames = []\n    tokens = [dictionary[c] for c in transcript.replace(\" \", \"\")]\n\n    targets = torch.tensor(tokens, dtype=torch.int32).unsqueeze(0)\n    input_lengths = torch.tensor([emission.shape[1]])\n    target_lengths = torch.tensor([targets.shape[1]])\n\n    # This is the key step, where we call the forced alignment API functional.forced_align to compute frame alignments.\n    frame_alignment, frame_scores = forced_align(emission, targets, input_lengths, target_lengths, 0)\n\n    assert frame_alignment.shape[1] == input_lengths[0].item()\n    assert targets.shape[1] == target_lengths[0].item()\n\n    token_index = -1\n    prev_hyp = 0\n    for i in range(frame_alignment.shape[1]):\n        if frame_alignment[0][i].item() == 0:\n            prev_hyp = 0\n            continue\n\n        if frame_alignment[0][i].item() != prev_hyp:\n            token_index += 1\n        frames.append(Frame(token_index, i, frame_scores[0][i].exp().item()))\n        prev_hyp = frame_alignment[0][i].item()\n\n    # compute frame alignments from token alignments\n    transcript_nospace = transcript.replace(\" \", \"\")\n    i1, i2 = 0, 0\n    segments = []\n    while i1 < len(frames):\n        while i2 < len(frames) and frames[i1].token_index == frames[i2].token_index:\n            i2 += 1\n        score = sum(frames[k].score for k in range(i1, i2)) / (i2 - i1)\n\n        segments.append(\n            Segment(\n                transcript_nospace[frames[i1].token_index],\n                frames[i1].time_index,\n                frames[i2 - 1].time_index + 1,\n                score,\n            )\n        )\n        i1 = i2\n\n    # compue word alignments from token alignments\n    separator = \" \"\n    words = []\n    i1, i2, i3 = 0, 0, 0\n    while i3 < len(transcript):\n        if i3 == len(transcript) - 1 or transcript[i3] == separator:\n            if i1 != i2:\n                if i3 == len(transcript) - 1:\n                    i2 += 1\n                s = 0\n                segs = segments[i1 + s : i2 + s]\n                word = \"\".join([seg.label for seg in segs])\n                score = sum(seg.score * seg.length for seg in segs) / sum(seg.length for seg in segs)\n                words.append(Segment(word, segments[i1 + s].start, segments[i2 + s - 1].end, score))\n            i1 = i2\n        else:\n            i2 += 1\n        i3 += 1\n\n    num_frames = frame_alignment.shape[1]\n    return segments, words, num_frames\n\n\n# utility function for plotting word alignments\ndef plot_alignments(segments, word_segments, waveform, input_lengths, scale=10):\n    fig, ax2 = plt.subplots(figsize=(64, 12))\n    plt.rcParams.update({\"font.size\": 30})\n\n    # The original waveform\n    ratio = waveform.size(1) / input_lengths\n    ax2.plot(waveform)\n    ax2.set_ylim(-1.0 * scale, 1.0 * scale)\n    ax2.set_xlim(0, waveform.size(-1))\n\n    for word in word_segments:\n        x0 = ratio * word.start\n        x1 = ratio * word.end\n        ax2.axvspan(x0, x1, alpha=0.1, color=\"red\")\n        ax2.annotate(f\"{word.score:.2f}\", (x0, 0.8 * scale))\n\n    for seg in segments:\n        if seg.label != \"|\":\n            ax2.annotate(seg.label, (seg.start * ratio, 0.9 * scale))\n\n    xticks = ax2.get_xticks()\n    plt.xticks(xticks, xticks / sample_rate, fontsize=50)\n    ax2.set_xlabel(\"time [second]\", fontsize=40)\n    ax2.set_yticks([])\n\n\n# utility function for playing audio segments.\n# A trick to embed the resulting audio to the generated file.\n# `IPython.display.Audio` has to be the last call in a cell,\n# and there should be only one call par cell.\ndef display_segment(i, waveform, word_segments, num_frames):\n    ratio = waveform.size(1) / num_frames\n    word = word_segments[i]\n    x0 = int(ratio * word.start)\n    x1 = int(ratio * word.end)\n    print(f\"{word.label} ({word.score:.2f}): {x0 / sample_rate:.3f} - {x1 / sample_rate:.3f} sec\")\n    segment = waveform[:, x0:x1]\n    return IPython.display.Audio(segment.numpy(), rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aligning multilingual data\n\nHere we show examples of computing forced alignments of utterances in\n5 languages using the multilingual Wav2vec2 model, with the alignments visualized.\nOne can also play the whole audio and audio segments aligned with each word, in\norder to verify the alignment quality. Here we first load the model and dictionary.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchaudio.models import wav2vec2_model\n\nmodel = wav2vec2_model(\n    extractor_mode=\"layer_norm\",\n    extractor_conv_layer_config=[\n        (512, 10, 5),\n        (512, 3, 2),\n        (512, 3, 2),\n        (512, 3, 2),\n        (512, 3, 2),\n        (512, 2, 2),\n        (512, 2, 2),\n    ],\n    extractor_conv_bias=True,\n    encoder_embed_dim=1024,\n    encoder_projection_dropout=0.0,\n    encoder_pos_conv_kernel=128,\n    encoder_pos_conv_groups=16,\n    encoder_num_layers=24,\n    encoder_num_heads=16,\n    encoder_attention_dropout=0.0,\n    encoder_ff_interm_features=4096,\n    encoder_ff_interm_dropout=0.1,\n    encoder_dropout=0.0,\n    encoder_layer_norm_first=True,\n    encoder_layer_drop=0.1,\n    aux_num_out=31,\n)\n\n\nmodel.load_state_dict(\n    torch.hub.load_state_dict_from_url(\n        \"https://dl.fbaipublicfiles.com/mms/torchaudio/ctc_alignment_mling_uroman/model.pt\"\n    )\n)\nmodel.eval()\n\n\ndef get_emission(waveform):\n    # NOTE: this step is essential\n    waveform = torch.nn.functional.layer_norm(waveform, waveform.shape)\n\n    emissions, _ = model(waveform)\n    emissions = torch.log_softmax(emissions, dim=-1)\n    emission = emissions.cpu().detach()\n\n    # Append the extra dimension corresponding to the <star> token\n    extra_dim = torch.zeros(emissions.shape[0], emissions.shape[1], 1)\n    emissions = torch.cat((emissions.cpu(), extra_dim), 2)\n    emission = emissions.detach()\n    return emission, waveform\n\n\n# Construct the dictionary\n# '@' represents the OOV token, '*' represents the <star> token.\n# <pad> and </s> are fairseq's legacy tokens, which're not used.\ndictionary = {\n    \"<blank>\": 0,\n    \"<pad>\": 1,\n    \"</s>\": 2,\n    \"@\": 3,\n    \"a\": 4,\n    \"i\": 5,\n    \"e\": 6,\n    \"n\": 7,\n    \"o\": 8,\n    \"u\": 9,\n    \"t\": 10,\n    \"s\": 11,\n    \"r\": 12,\n    \"m\": 13,\n    \"k\": 14,\n    \"l\": 15,\n    \"d\": 16,\n    \"g\": 17,\n    \"h\": 18,\n    \"y\": 19,\n    \"b\": 20,\n    \"p\": 21,\n    \"w\": 22,\n    \"c\": 23,\n    \"v\": 24,\n    \"j\": 25,\n    \"z\": 26,\n    \"f\": 27,\n    \"'\": 28,\n    \"q\": 29,\n    \"x\": 30,\n    \"*\": 31,\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before aligning the speech with transcripts, we need to make sure\nthe transcripts are already romanized. Here are the BASH commands\nrequired for saving raw transcript to a file, downloading the uroman\nromanizer and using it to obtain romanized transcripts, and PyThon\ncommands required for further normalizing the romanized transcript.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\nSave the raw transcript to a file\necho 'raw text' > text.txt\ngit clone https://github.com/isi-nlp/uroman\nuroman/bin/uroman.pl < text.txt > text_romanized.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\nimport re\ndef normalize_uroman(text):\n    text = text.lower()\n    text = text.replace(\"\u2019\", \"'\")\n    text = re.sub(\"([^a-z' ])\", \" \", text)\n    text = re.sub(' +', ' ', text)\n    return text.strip()\n\nfile = \"text_romanized.txt\"\nf = open(file, \"r\")\nlines = f.readlines()\ntext_normalized = normalize_uroman(lines[0].strip())\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### German example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "text_raw = (\n    \"aber seit ich bei ihnen das brot hole brauch ich viel weniger schulze wandte sich ab die kinder taten ihm leid\"\n)\ntext_normalized = (\n    \"aber seit ich bei ihnen das brot hole brauch ich viel weniger schulze wandte sich ab die kinder taten ihm leid\"\n)\nspeech_file = torchaudio.utils.download_asset(\"tutorial-assets/10349_8674_000087.flac\", progress=False)\nwaveform, _ = torchaudio.load(speech_file)\n\nemission, waveform = get_emission(waveform)\nassert len(dictionary) == emission.shape[2]\n\ntranscript = text_normalized\n\nsegments, word_segments, num_frames = compute_alignments(transcript, dictionary, emission)\nplot_alignments(segments, word_segments, waveform, emission.shape[1])\n\nprint(\"Raw Transcript: \", text_raw)\nprint(\"Normalized Transcript: \", text_normalized)\nIPython.display.Audio(waveform, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(0, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(1, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(2, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(3, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(4, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(5, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(6, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(7, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(8, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(9, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(10, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(11, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(12, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(13, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(14, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(15, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(16, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(17, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(18, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(19, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(20, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chinese example:\n\nChinese is a character-based language, and there is not explicit word-level\ntokenization (separated by spaces) in its raw written form. In order to\nobtain word level alignments, you need to first tokenize the transcripts\nat the word level using a word tokenizer like [\u201cStanford\nTokenizer\u201d](https://michelleful.github.io/code-blog/2015/09/10/parsing-chinese-with-stanford/)_.\nHowever this is not needed if you only want character-level alignments.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "text_raw = \"\u5173 \u670d\u52a1 \u9ad8\u7aef \u4ea7\u54c1 \u4ecd \u5904\u4e8e \u4f9b\u4e0d\u5e94\u6c42 \u7684 \u5c40\u9762\"\ntext_normalized = \"guan fuwu gaoduan chanpin reng chuyu gongbuyingqiu de jumian\"\nspeech_file = torchaudio.utils.download_asset(\"tutorial-assets/mvdr/clean_speech.wav\", progress=False)\nwaveform, _ = torchaudio.load(speech_file)\nwaveform = waveform[0:1]\n\nemission, waveform = get_emission(waveform)\n\ntranscript = text_normalized\n\nsegments, word_segments, num_frames = compute_alignments(transcript, dictionary, emission)\nplot_alignments(segments, word_segments, waveform, emission.shape[1])\n\nprint(\"Raw Transcript: \", text_raw)\nprint(\"Normalized Transcript: \", text_normalized)\nIPython.display.Audio(waveform, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(0, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(1, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(2, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(3, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(4, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(5, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(6, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(7, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(8, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Polish example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "text_raw = \"wtedy ujrza\u0142em na jego brzuchu okr\u0105g\u0142\u0105 czarn\u0105 ran\u0119 dlaczego mi nie powiedzia\u0142e\u015b szepn\u0105\u0142em ze \u0142zami\"\ntext_normalized = \"wtedy ujrzalem na jego brzuchu okragla czarna rane dlaczego mi nie powiedziales szepnalem ze lzami\"\nspeech_file = torchaudio.utils.download_asset(\"tutorial-assets/5090_1447_000088.flac\", progress=False)\nwaveform, _ = torchaudio.load(speech_file)\n\nemission, waveform = get_emission(waveform)\n\ntranscript = text_normalized\n\nsegments, word_segments, num_frames = compute_alignments(transcript, dictionary, emission)\nplot_alignments(segments, word_segments, waveform, emission.shape[1])\n\nprint(\"Raw Transcript: \", text_raw)\nprint(\"Normalized Transcript: \", text_normalized)\nIPython.display.Audio(waveform, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(0, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(1, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(2, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(3, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(4, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(5, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(6, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(7, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(8, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(9, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(10, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(11, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(12, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(13, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(14, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Portuguese example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "text_raw = (\n    \"mas na imensa extens\u00e3o onde se esconde o inconsciente imortal s\u00f3 me responde um bramido um queixume e nada mais\"\n)\ntext_normalized = (\n    \"mas na imensa extensao onde se esconde o inconsciente imortal so me responde um bramido um queixume e nada mais\"\n)\nspeech_file = torchaudio.utils.download_asset(\"tutorial-assets/6566_5323_000027.flac\", progress=False)\nwaveform, _ = torchaudio.load(speech_file)\n\nemission, waveform = get_emission(waveform)\n\ntranscript = text_normalized\n\nsegments, word_segments, num_frames = compute_alignments(transcript, dictionary, emission)\nplot_alignments(segments, word_segments, waveform, emission.shape[1])\n\nprint(\"Raw Transcript: \", text_raw)\nprint(\"Normalized Transcript: \", text_normalized)\nIPython.display.Audio(waveform, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(0, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(1, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(2, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(3, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(4, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(5, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(6, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(7, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(8, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(9, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(10, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(11, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(12, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(13, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(14, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(15, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(16, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(17, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(18, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(19, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Italian example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "text_raw = \"elle giacean per terra tutte quante fuor d'una ch'a seder si lev\u00f2 ratto ch'ella ci vide passarsi davante\"\ntext_normalized = (\n    \"elle giacean per terra tutte quante fuor d'una ch'a seder si levo ratto ch'ella ci vide passarsi davante\"\n)\nspeech_file = torchaudio.utils.download_asset(\"tutorial-assets/642_529_000025.flac\", progress=False)\nwaveform, _ = torchaudio.load(speech_file)\n\nemission, waveform = get_emission(waveform)\n\ntranscript = text_normalized\n\nsegments, word_segments, num_frames = compute_alignments(transcript, dictionary, emission)\nplot_alignments(segments, word_segments, waveform, emission.shape[1])\n\nprint(\"Raw Transcript: \", text_raw)\nprint(\"Normalized Transcript: \", text_normalized)\nIPython.display.Audio(waveform, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(0, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(1, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(2, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(3, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(4, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(5, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(6, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(7, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(8, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(9, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(10, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(11, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(12, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(13, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(14, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(15, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(16, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display_segment(17, waveform, word_segments, num_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this tutorial, we looked at how to use torchaudio\u2019s forced alignment\nAPI and a Wav2Vec2 pre-trained mulilingual acoustic model to align\nspeech data to transcripts in five languages.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Acknowledgement\n\nThanks to [Vineel Pratap](vineelkpratap@meta.com)_ and [Zhaoheng\nNi](zni@meta.com)_ for working on the forced aligner API, and\n[Moto Hira](moto@meta.com)_ for providing alignment merging and\nvisualization utilities.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}